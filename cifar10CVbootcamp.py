# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IV5oM6hxbsQRkPQ6oywtzlzohNweLpcy
"""

from keras import layers
from keras import models
model = models.Sequential() 
model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3))) 
model.add(layers.Conv2D(64, (3, 3),  padding='same', kernel_initializer='he_uniform', activation='relu'))
model.add(layers.MaxPooling2D((2, 2))) 
model.add(layers.Dropout(0.2))
model.add(layers.Conv2D(64, (3, 3),  padding='same', kernel_initializer='he_uniform', activation='relu')) 
model.add(layers.Conv2D(128, (3, 3),  padding='same', kernel_initializer='he_uniform', activation='relu'))
model.add(layers.MaxPooling2D((2, 2))) 
model.add(layers.Dropout(0.2)) 
model.add(layers.Flatten())
model.add(layers.Dense(128,  activation='relu'))
model.add(layers.Dense(10, activation='softmax'))

"""created convolutional base following 2 blocks of Conv2D-MaxPooling2D with Dropout layers to reduce over-fitting"""

from keras import optimizers
model.compile(loss='categorical_crossentropy', optimizer='adam',
                      metrics=['acc'])

"""compiled model using categorical cross-entrophy as the loss function and the Adam optimizer"""

from keras.datasets import cifar10
from keras.utils import to_categorical

(inpTrain, tarTrain), (inpTest, tarTest) = cifar10.load_data()
tarTrain = to_categorical(tarTrain)
tarTest = to_categorical(tarTest)

"""downloaded dataset and prepared it for processing"""

from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
tempInpTrain = inpTrain.astype('float32')
tempInpTest= inpTest.astype('float32')

newInpTrain= tempInpTrain / 255.0
newInpTest = tempInpTest / 255.0
genData = ImageDataGenerator(rotation_range=40,width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)
iterTrain = genData.flow(newInpTrain, tarTrain, batch_size=64)

"""used data augmentation to further reduce overfitting by applying transformations to the images"""

history = model.fit_generator(iterTrain, steps_per_epoch=700, epochs=50, validation_data=(newInpTest, tarTest))

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

test_loss, test_acc = model.evaluate(newInpTest,  tarTest)

"""The model reached an accuracy of 78.33% with a loss of 0.6912. The graphs are also demonstrate that using both Dropout layers and data augmentation helped reduce the chances of the model overfitting. 

There is room for improvement to get higher accuracy values. This model was fairly simple with only 2 Conv2D blocks, but using more blocks would improve the overall model, though as the model becomes more complex training time takes longer. Alternatively fine-tuning a pretrained model such as InceptionV3 would see drastic improvements in performance.
"""

